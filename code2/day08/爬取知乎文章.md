## 爬取知乎文章 初识爬虫
### 1.回顾前路
### 2.项目实操

总体上来说，从`Response`对象开始，我们就分成了两条路径，一条路径是数据放在HTML里，所以我们用`BeautifulSoup`库去解析数据和提取数据；另一条，数据作为Json存储起来，所以我们用`response.json()`方法去解析，然后提取、存储数据。

- 0.明确目标

爬取知乎大v张佳玮的文章“标题”、“摘要”、“链接”，并存储到本地文件。

- 1.分析过程

张佳玮的知乎文章URL在这里：https://www.zhihu.com/people/zhang-jia-wei/posts?page=1。（为了尽可能让你看到的内容和我一致，我选择了按赞同数给文章排序）

点击右键——检查——`Network`，选`All`（而非XHR），然后刷新网页，点进去第0个请求:`posts_by_votes`，点`Preview`。

- 2.代码实现

原来，前10篇里，也只有前2篇在静态HTML中，实际上，老师又刨根问底了一次，发现其它8篇文章以json数据的形式储存在了<script id="js-initialData" type="text/json">标签当中

- 1.重新分析

我们打开`Network`，点开`XHR`，同时刷新页面，看到出现了很多个请求。

浏览一下，看到两个带`articles`的请求，感觉有戏。点开首个`articles`看看`preview`，一层层点开，看到`“title：记住就是一切”`，猜测这是一个文章标题。

在网页里面用command+f(windows电脑用ctrl+f)搜索一下“记住就是一切”，发现搜不到，奇怪。

那就看看跟首个`articles`请求长得很像的另一个`articles`的请求好啦，仍然看`preview`，看到`title: "国产航母下水……让我想到李鸿章和北洋舰队"`，仍然在网页里搜一下：

哎鸭！果然在这里。看来这个articles的请求里面存的是第一页的文章标题。这下妥了，我们知道从哪个`url`获取数据了。从`data`结构来看，刚好就是后10篇。

那首个带`articles`的请求是什么？其实这是知乎的网站设计，当你刷新第一页的时候，默认你也请求了第二页的文章数据，这样你加载就会比较流畅。

通过同样的方法，我们还可以发现文章的链接、文章的摘要也一样放在`articles`的请求里，这里就不赘述了。

然后发现除了offset都一样，offset代表偏移量，和页码有对应关系，通过循环遍历各页我们就可以爬到所有页数的内容了。

- 2.代码实现
们可以参考上面的知识地图，一步一步来写，所以先请你用`requests.get()`获取数据，然后检查请求是否成功。

 [参考代码](demo02.py)

至此，第一页的数据我们就拿到了，接下来要去拿所有页面的数据。

我们在上一步已经分析过了，第一页和第二页的请求的参数区别在于offset，再看一下第三页、第四页，找一下规律，由此可以写一个循环。

那么如何结束这个循环呢？

这里老师有一个经验之谈，我们来看看第一页和最后一页data部分下面的paging的参数区别：

对比一下，你会发第一页的is_end是显示false，最后一页的is_end是显示true，这个元素可以帮我们结束循环。

至于那个totals: 919元素，我算了一下页码和每页的文章数，判断这是文章的总数，也同样可以作为结束循环的条件。

以上告诉你我是怎么做的，现在聊聊我是怎么想到的。我刚才的思路其实和找合适的标签提取标题时很像，类似“data里有我们的目标数据，但好像没有结束循环的标志，视野稍稍放宽，找一找上下文有没有帮助我们结束循环的数据。”

所谓“经验之谈”，就是这样的知识迁移了。在熟练运用之前我们还有其他思路可参考，比如，文章是分页的，按页码来嘛。如果够细心，你可能还会发现知乎的一个设定：按点赞数排序的文章只显示前**页。（什么？数字部分打码了？不想被剧透先自己找找看吧。）

理论上，我们就可以拿到按赞同数排序的这25页数据了。但是在这里，我们并不建议爬取全部数据。大家想想，参加课程的同学有很多，每个人都去爬这25页的话，如此高频访问会给对方的服务器造成相当大的压力，而我们并不真正需要这么多的数据，只是为了学习爬虫。

这里还涉及到一个更严肃的问题：在合法使用爬虫时，除了遵循robots协议，还要注意避免超大量爬取对网站服务器造成过大负担。

所以，我们的循环会设置为爬两页数据就停止。代码如下：[代码](./demo04.py)

好啦，你读完这段代码就该你去写啦：

获取、解析、提取数据都完成了。好，接下来就是存储数据了。

在我的想象中，最终的数据最好存成下面这样简明清晰的表格：

`csv`和`openpyxl`都可以做到，但在这里我们会选取`csv`，因为上一关的案例是用的`openpyxl`，那我们今天就来试试`csv`。

上一关，我们学的csv最主要的功能就是可以把列表按行写入。

反推回来，也就是我们如果可以把数据写成`list=[title,url,excerpt]`的样子，那就可以直接写入啦。

刚刚我们爬取到的数据也是这样的一个列表，那么我们就知道如何写代码了。

[代码](./demo05.py)
(≧▽≦)/ 充满了喜悦。

代码实现我们就搞完了，开心吖~
### 3.展望未来

来到这里，你的爬虫就已经入门了！！！

接下来的关卡，你会学习更厉害的爬虫技能，以面对更多更复杂的情况。

第8关是运用`cookies`让浏览器记住我们，
第9关是用`selenium`控制浏览器，
第10关是让爬虫程序能定时向我们汇报结果，
第11-14关是运用协程和`scrapy`框架来帮我们提速，并且可以爬取海量的数据，
第15关是复习。
