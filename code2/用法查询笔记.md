> ## 用法查询笔记-Python爬虫精进

## 目标1初窥门径
>### 第0关 认识爬虫
`2019年12月31日` 重新定义网上冲浪
```python
import requests
url = 'http://baidu.com'
res = requests.get(url)
print(res.text)
```

>### 第1关 HTML基础
`2020年1月1日` 课堂: 我也可以写个网页

## 目标二：爬虫小成
>### 第2关 BeautifulSoup
`2020年1月1日` 课堂: 爬虫初体验


### BeautifulSoup怎么用
`BeautifulSoup`库目前已经进阶到第4版（Beautiful Soup 4），由于它不是Python标准库，而是第三方库，需要单独安装它，不过，我们的学习系统已经安装好了。

如果你是在自己的电脑上运行，需要在终端输入一行代码运行：`pip install BeautifulSoup4`。（Mac电脑需要输入`pip3 install BeautifulSoup4`）

- 解析数据

```python
import requests
# 引入BS库，下面的bs4就是beautifulsoup4
from bs4 import BeautifulSoup
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html') 
html = res.text
# 把网页解析为BeautifulSoup对象
soup = BeautifulSoup(html,'html.parser') 
print(soup)  #输出 <html>...</html>
print(type(soup))  #输出 <class 'bs4.BeautifulSoup'>
```

- 提取数据

我们仍然使用`BeautifulSoup`来提取数据。

又可以分为两部分知识：find()与find_all()，以及Tag对象（标签对象）。

首先，请看举例中括号里的class_，这里有一个下划线，是为了和python语法中的类 class区分，避免程序冲突。当然，除了用class属性去匹配，还可以使用其它属性，比如style属性等。

数据类型是<class 'bs4.element.ResultSet>， 前面说过可以把它当做列表list来看待。

>### 第3关 BeautifulSoup实践
`2020年1月1日` 课堂：解密吴氏私厨
- 练习 豆瓣爬虫
```python
import requests, bs4
# 为躲避反爬机制，伪装成浏览器的请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}
url = 'http://baidu.com'
res = requests.get(url, headers=headers)
```
- 进阶练习 一键下载电影
```python
from urllib.parse import quote
a= '无名之辈'
b= a.encode('gbk')
# 将汉字，用gbk格式编码，赋值给b
print(quote(b))  # %CE%DE%C3%FB%D6%AE%B1%B2
# quote()函数，可以帮我们把内容转为标准的url格式，作为网址的一部分打开
```

>### 第4关 json
`2020年1月1日` 课堂：寻找周杰伦
```python
import requests
r = requests.get("http://....")
print(r.json())
```
而如果你想在Python语言中，实现列表/字典转json，json转列表/字典，则需要借助json模块。json模块不在我们的教学范围之内，所以不做展开。你可阅读它的官方文档来了解，地址在这里：https://docs.python.org/3/library/json.html
```python
# 引入json模块
import json
# 创建一个列表a
a = [1,2,3,4]
# 使用dumps()函数，将列表a转换为json格式的字符串，赋值给b
b = json.dumps(a)
# 打印b
print(b)
# 打印b的数据类型
print(type(b))
# 使用loads()函数，将json格式的字符串b转为列表，赋值给c
c = json.loads(b)
# 打印c
print(c)
# 打印c的数据类型
print(type(c)) 
```

>### 第5关 带参数请求数据
`2020年1月1日` 课堂：狂热粉丝

设置请求头和简化请求参数
```python
import requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}
url = 'http://baidu.com'
# 请求参数是根据当前请求需要的参数来定的
params = {'pagenum':'1'}
res = requests.get(url, headers=headers,params=params)
```


>### 第6关 csv&excel
`2020年1月1日` 课堂：爬到的数据存哪里？

```python
# csv写入的代码：

import csv
csv_file = open('demo.csv','w',newline='')
writer = csv.writer(csv_file)
writer.writerow(['电影','豆瓣评分'])
csv_file.close()
```
```python
# csv读取的代码：

import csv
csv_file = open('demo.csv','r',newline='')
reader=csv.reader(csv_file)
for row in reader:
    print(row)
```
```python
# Excel写入的代码：

import openpyxl 
wb = openpyxl.Workbook() 
sheet = wb.active
sheet.title ='new title'
sheet['A1'] = '漫威宇宙'
rows = [['美国队长','钢铁侠','蜘蛛侠','雷神'],['是','漫威','宇宙', '经典','人物']]
for i in rows:
    sheet.append(i)
print(rows)
wb.save('Marvel.xlsx')
```
```python
# Excel读取的代码：

import openpyxl
wb = openpyxl.load_workbook('Marvel.xlsx')
sheet = wb['new title']
sheetname = wb.sheetnames
print(sheetname)
A1_value = sheet['A1'].value
print(A1_value)
```

>### 第7关 爬取知乎文章
`2020年1月1日` 课堂：复习：温故而知新

## 目标三：更上层楼

>### 第8关 cookies
`2020年1月1日` 课堂：带着小饼干登录

`post`请求和`cookie`
[demo03](./day08/demo03.py)
- 储存 cookie

cookies 转化成字典的方法
```python
import json
json_str = json.dumps({"a":123})
```

使用 `session` 参考 [demo04](./day08/demo04.py), [demo05](./day08/demo05.py)

参考代码 [demo06](./day08/demo06.py), [demo07](./day08/demo07.py), [demo08](./day08/demo08.py)

- 读取cookies
```python
import requests
import json
session = requests.session()
cookies_txt = open('cookies.txt', 'r')
#以reader读取模式，打开名为cookies.txt的文件。
cookies_dict = json.loads(cookies_txt.read())
#调用json模块的loads函数，把字符串转成字典。
cookies = requests.utils.cookiejar_from_dict(cookies_dict)
#把转成字典的cookies再转成cookies本来的格式。
session.cookies = cookies
#获取cookies：就是调用requests对象（session）的cookies属性。
```
```python
import requests
import json
def cookies_read():
    cookies_txt = open('cookies.txt', 'r')
    cookies_dict = json.loads(cookies_txt.read())
    cookies = requests.utils.cookiejar_from_dict(cookies_dict)
    return (cookies)
    # 以上4行代码，是cookies读取。
```
练习_小说推荐
```python
# 过滤器fliter过滤数字
link = 'https://www.xslou.com/yuedu/9356/'

# 字符串link过滤出数字id（9356）
id_list = list(filter(str.isdigit,link))
book_id = ''.join(id_list)

# 步骤解析：1、filter()过滤数字 2、filter对象转列表 3、列表转字符串 
# filter(str.isdigit,字符串) 
# 第一个参数用来判断字符串的单个元素是否是数字，数字保留
# filter()返回的是对象，需要用list()函数转换成列表
# ''.join(列表)将列表转换成字符串

# 方法二
href='https://www.xslou.com/yuedu/9356/'
# 分割最后位置的数字,拿到book_id
book_id = href.split('/')[-2]
```

>### 第9关 Selenium
`2020年1月1日` 课堂：指挥浏览器自动工作

>### 第10关 定时与邮件
`2020年1月1日` 课堂：让爬虫按时向你汇报

## 目标四：拨云见日
>### 第11关 协程
`2020年1月1日` 课堂：建立爬虫军队

>### 第12关 协程实践
`2020年1月1日` 课堂：吃什么不会胖？

>### 第13关 Scrapy框架
`2020年1月1日` 课堂：各司其职的爬虫公司

>### 第14关 Scrapy实操
`2020年1月1日` 课堂：出任爬虫公司CEO

>### 第15关 复习与反爬虫
`2020年1月1日` 课堂：逢山开路与不甘庸碌
