## 带参数请求数据 狂热粉丝

> 1.复习
> 2.项目:狂热粉丝
> 3.分析过程

你可能会疑惑，为什么不能直接爬取周杰伦的更多歌曲，来吧，先来打开偶像的qq音乐搜索链接：

https://y.qq.com/portal/search.html#page=1&searchid=1&remoteplace=txt.yqq.top&t=song&w=%E5%91%A8%E6%9D%B0%E4%BC%A6

如果仔细观察页面的内容，你可能会摇摇头，认为爬取更多的歌曲信息是一个不可能完成的任务，因为如今的qq音乐已经不支持更多歌曲的翻阅功能，正常访问的途径都没有了，我们的“虫”（代码）上哪爬取去？

退而求其次，咱们来看看评论这边的情况，目前qq音乐的评论页面结构是这样的：

评论区分成了两部分，分别是精彩评论和最新评论，精彩评论可以通过点击加载更多获得更多数据，最新评论则需要翻页来进行数据更新。

也就是说，爬取更多的评论数据的难点似乎在翻页和点击加载更多。

显然这种数据的加载模式还是我们上一关熟悉的“动态加载”，即点击一个按钮（加载更多或者第n页），服务器就会根据新的XHR更新页面信息。

这样一来，若我们想要获得更多的评论数据，就需要：1️⃣点击按钮获得新的XHR 2️⃣根据新的XHR获得json数据 3️⃣解析json数据，如此循环往复，直至精神崩溃。

如果python有情绪，它的脸上现在肯定写满了嫌弃和拒绝。所以，看在python的面上，只要我们能找到每个请求之间的规律，这种重复的工作就交给它了好吗？

相比较爬取我们“看不见”（更多歌曲信息）的数据，评论这个能通过点击页码或加载更多按钮看到更多数据的“柿子”看起来比较软一点，就先捏它吧~

不过，解决评论的问题，需要用到“带参数请求数据”。学会这个，所有的问题都不再是问题。

- 什么是带参数请求数据


在上面，我们能看到每个url都由两部分组成。前半部分大多形如：https://xx.xx.xxx/xxx/xxx

后半部分，多形如：xx=xx&xx=xxx&xxxxx=xx&……

两部分使用?来连接。举例刚刚的豆瓣网址，前半部分就是：

https://www.douban.com/search

后半部分则是：q=%E6%B5%B7%E8%BE%B9%E7%9A%84%E5%8D%A1%E5%A4%AB%E5%8D%A1

它们的中间使用了?来隔开。

这前半部分是我们所请求的地址，它告诉服务器，我想访问这里。而后半部分，就是我们的请求所附带的参数，它会告诉服务器，我们想要什么样的数据。

这参数的结构，会和字典很像，有键有值，键值用=连接；每组键值之间，使用&来连接。

就像豆瓣。我们请求的地址是https://www.douban.com/search

而我们的请求所附带的参数是“海边的卡夫卡”：q=%E6%B5%B7%E8%BE%B9%E7%9A%84%E5%8D%A1%E5%A4%AB%E5%8D%A1(那段你看不懂的代码，它是“海边的卡夫卡”使用utf-8编码的结果)。

技能点学满了吧？那现在，我们要以《七里香》为例，爬取用户的精彩评论。

首先，进入网址：

https://y.qq.com/n/yqq/song/004Z8Ihr0JIu5s.html

打开Network，选中All，点击刷新。

这次的XHR还挺多，有四五十个。常规来说我们有两种方法来寻找XHR：阅读它们的name看看哪个可能是评论；或者是一个一个翻。

现在再给你介绍一个简单的小技巧：先把Network面板清空，再点击一下精彩评论的点击加载更多，看看有没有多出来的新XHR，多出来的那一个，就应该是和评论相关的啦。

我们点开这个请求的Preview，能够在`['comment']['commentlist']`里找到评论列表。列表的每一个元素都是字典，字典里键`rootcommentcontent`对应的值，就是我们要找的评论。

好嘞，于是我们就在找到拥有评论数据的页面链接（请求的Headers栏：General中的Request URL）：

https://c.y.qq.com/base/fcgi-bin/fcg_global_comment_h5.fcg?g_tk=5381&loginUin=0&hostUin=0&format=json&inCharset=utf8&outCharset=GB2312&notice=0&platform=yqq.json&needNewCode=0&cid=205360772&reqtype=2&biztype=1&topid=102065756&cmd=6&needmusiccrit=0&pagenum=1&pagesize=15&lasthotcommentid=song_102065756_3202544866_44059185&domain=qq.com&ct=24&cv=10101010

显然，这样一个长链接，阅读体验非常之差。Network面板提供了一个更友好的查看方式，我来带你看看它。

回到上面我们找到XHR的地方，选中Headers，保持General打开，保持Response Headers和Request Headers关闭。我们点开Query String Parameters。

它里面的内容有没有感觉眼熟？正是链接请求中所附带的参数，对吧！Query String Parameters，它的中文翻译是：查询字符串参数。

这个面板用类似字典的形式，呈现了各个参数的键值，阅读体验会好一些。我们可以常来此处看看。

- 如何带参数请求数据

在上一关当中，我们直接用`requests.get()`请求了`url`。在这一关，如果我们想拿到更多精彩评论。就要读懂`url`的各个参数。修改它们，重新发起请求。

读懂参数，有两个重要的方法是“观察”和“比较”。“观察”指的是阅读参数的键与值，尝试理解它的含义。“比较”指的是比较两个相近的XHR——它们有哪些不同，对应的页面显示内容有什么不同。

现在，我们来观察比较，依然在“七里香”的歌曲详情页，点击精彩评论的点击加载更多按钮，此时`Network`会多加载出更多的XHR，但是`Name`为`fcg_global_comment_h5…`才是我们关心的XHR。

分别点开它们的`Query String Parameters`，比较参数之间有什么不同。点击`Enter（回车）`键，告诉你答案。

事实上答案已经很明显了，只要我们多点耐心就会发现，链接的众多参数中，只有一个参数在变化。这个参数是pagenum，第一次点击加载更多的值为1，第二第三次点击它的值就变成了2和3。

当然，pagenum这个复合英文本身也说明了问题，指的可不就是页码嘛！也就是说，pagenum=1等于告诉服务器：我要歌曲信息列表第一页的数据，pagenum=2：我要歌曲信息列表第二页的数据。

这样一来，按照之前学的知识，你大约会想：我们写一个循环，每次循环都去更改pagenum的值，这样不就能实现爬取好多好多精彩评论了吗？

> 4.代码实现

这样想是完全正确的。你可以尝试这样做。来，改写下方代码，实现这一点。备注：虽然这样能爬到我们的数据，但我并不推荐你在课堂里把所有的评论都爬取下来。一来是等待时间较长，二来虽然QQ音乐没有对爬虫做限制，但是我们也应该尽可能减少对服务器的压力。所以，循环不要超过5个。
```python
#参考demo01.py
```
这样写代码，的确能够完成我们的目标。但是，这样写代码修改链接的参数太麻烦了，显然不够优雅，因为它是在太长了。

素有“人工智能之父”的唐纳德·克努特（Donald Ervin Knuth）讲过一句话让我印象深刻，他说：代码写出来是让人读的，只是顺便拿去让机器执行。

我们来让这个代码变好看些。事实上，`requests`模块里的`requests.get()`提供了一个参数叫`params`，可以让我们用字典的形式，把参数传进去。它的官方文档，是这样描述：

所以，其实我们可以把Query String Parameters里的内容，直接复制下来，封装为一个字典，传递给params。只是有一点要特别注意：要给他们打引号，让它们变字符串。

所以，代码最后可能长这样：
```python
#参考demo02.py
```
> 5.被隐藏的歌曲清单

好了，现在回到一开始遇到的难题：我想要爬取周杰伦更多的歌曲信息，但是qq音乐告诉我：想要查看更多内容，请下载一个客户端。

前面我们提到，一个完整url是由两部分组成的，前半部分大多形如：`https://xx.xx.xxx/xxx/xxx`，后半部分则是`xx=xx&xx=xxx&xxxxx=xx&`，中间有时候会用?来连接。

来看看我们搜索的首页:

https://y.qq.com/portal/search.html#page=1&searchid=1&remoteplace=txt.yqq.top&t=song&w=周杰伦

不出所料，这个链接的前半部分是`https://y.qq.com/portal/search.html`，后半部分是`page=1&searchid=1&remoteplace=txt.yqq.top&t=song&w=周杰伦`，然而，分隔这两部分的符号不是`?`，而是`#`。

其实在这里，#和?的功能是一样的，作用都是分隔，若把链接的#替换成?，访问的效果是一样的（注意：用?分隔的url不一定可以用#代替）。

既然如此，我们是不是可以跟前面一样，对参数下手了呢？

观察一下后半部分的参数`page=1&searchid=1&remoteplace=txt.yqq.top&t=song&w=周杰伦`，page（中文：页面），`searchid（中文：搜索id）`，`remoteplace（中文：远程位置）`，后面的t和w这俩参数虽然不知道是什么，但根据他们的值（song和周杰伦）可窥得一斑，应该是指类型和关键字。

前面我们在爬取评论的时候知道，改变pagenum就可以加载更多的数据。举一反三，试想，如果改变搜索页面（`https://y.qq.com/portal/search.html#page=1&searchid=1&remoteplace=txt.yqq.top&t=song&w=周杰伦`）的page这个参数我们是否可以访问到其他页面的数据呢？

为了验证猜想，老师将网页链接中的`page=1`改成了`page=2`，果然就访问到了下一的数据，嘿嘿嘿，来吧，可以造作起来了！

还记得更快查找XHR的骚操作吗？1️⃣先把Network面板清空，2️⃣再修改page值按回车键，3️⃣查看`Network`多出来的新`XHR`，也就是这个`client_search_cp..`。

剩下的事情就简单了，重复上面的步骤，找到`client_search_cp..`，点开`Query String Parameters`，观察参数的变化规律。

找到了吗？这次也只有一个参数在变化哦~

这个参数是p，第1页XHR的参数p值为1，第2、3页XHR的参数p值则为2和3，说明在这个`client_search_cp..`的请求中，代表页码的参数是p（page的缩写）。

依然是爬取歌曲的相关信息（歌曲名、所属专辑、播放时长、播放链接），只不过这一次，可以爬取的可不止是第一页的数据。

如此，代码应该如下（同上，不推荐循环超过5次）：

悄悄地告诉你，如果你将这个代码里'w'关键字参数值换成另一个歌手/歌曲名，那么它也能爬到这个歌手/同名歌曲的信息。如果你愿意，可以在本关卡结束后，练习做这件事。

当然，qq音乐的产品经理肯定是不希望我们能访问到第2页的内容，他们更希望我们能下载客户端，从客户端访问数据。

为此，服务器就可能会对我们这些“投机取巧”的爬虫做限制处理。一来可以降低服务器的访问压力，毕竟成千上万次的访问对代码来说就是一个for循环的事儿；二来可以拦截那些想要通过爬虫窃取数据的竞争者。

那这就有一个问题，服务器怎么判断访问者是一个普通的用户（通过浏览器），还是一个爬虫者（通过代码）呢？

这需要我们回到浏览器中，重新认识一个新的信息栏：请求头`Request Headers`。

- 什么是Requests Headers

每一个请求，都会有一个`Requests Headers`，我们把它称作请求头。它里面会有一些关于该请求的基本信息，比如：这个请求是从什么设备什么浏览器上发出？这个请求是从哪个页面跳转而来？

如上图，`user-agent`（中文：用户代理）会记录你电脑的信息和浏览器版本（如我的，就是windows10的64位操作系统，使用谷歌浏览器）。

origin（中文：源头）和referer（中文：引用来源）则记录了这个请求，最初的起源是来自哪个页面。它们的区别是`referer`会比`origin`携带的信息更多些。

如果我们想告知服务器，我们不是爬虫是一个正常的浏览器，就要去修改`user-agent`。倘若不修改，那么这里的默认值就会是Python，会被浏览器认出来。

有趣的是，像百度的爬虫，它的`user-agent`就会是`Baiduspider`，谷歌的也会是`Googlebot……`如是种种。

而对于爬取某些特定信息，也要求你注明请求的来源，即`origin`或`referer`的内容。比如我有试过，在爬取歌曲详情页里的歌词时，就需要注明这个信息，否则会拿不到歌词。你可以在写练习的时候进行尝试。

- 如何添加Requests Headers

`Requests`模块允许我们去修改`Headers`的值。点击它的官方文档，搜索“user-agent”，你会看到：

如上，只需要封装一个字典就好了。和写params非常相像。

而修改origin或referer也和此类似，一并作为字典写入headers就好。就像这样：

```python
import requests
url = 'https://c.y.qq.com/soso/fcgi-bin/client_search_cp'

headers = {
    'origin':'https://y.qq.com',
    # 请求来源，本案例中其实是不需要加这个参数的，只是为了演示
    'referer':'https://y.qq.com/n/yqq/song/004Z8Ihr0JIu5s.html',
    # 请求来源，携带的信息比“origin”更丰富，本案例中其实是不需要加这个参数的，只是为了演示
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
    # 标记了请求从什么设备，什么浏览器上发出
    }
# 伪装请求头

params = {
'ct':'24',
'qqmusic_ver': '1298',
'new_json':'1',
'remoteplace':'sizer.yqq.song_next',
'searchid':'64405487069162918',
't':'0',
'aggr':'1',
'cr':'1',
'catZhida':'1',
'lossless':'0',
'flag_qc':'0',
'p':1,
'n':'20',
'w':'周杰伦',
'g_tk':'5381',
'loginUin':'0',
'hostUin':'0',
'format':'json',
'inCharset':'utf8',
'outCharset':'utf-8',
'notice':'0',
'platform':'yqq.json',
'needNewCode':'0'    
}
# 将参数封装为字典
res_music = requests.get(url,headers=headers,params=params)
# 发起请求，填入请求头和参数
```

鉴于这个代码你已经写过也运行过，所以在这里我们不再做练习。 只需要记得Request Headers（请求头）的含义和用法就好。

如果有一天，你真的需要爬取一万多条信息，将for循环执行成百上千次。

那么，你最好将自己的爬虫伪装成真实的浏览器（填写请求头）—— 因为在那种情况下，服务器很可能拒绝爬虫访问。甚至有的网站，一开始就不允许爬虫访问。如，知乎、猫眼电影。


> 6.复习

在本关卡，我们主要学习了带参数请求数据和Request Headers的用法。

对于前者，我们认识到一个url由两部分组成，?（有时候是“#”）之前是我们请求的地址，?之后是我们的请求所附带的参数。通常，我们会把参数封装成一个字典，添加进请求中去。

通过对参数进行修改，我们能爬到许多信息。

大约在第8关，我们还会学习另一种数据请求的类型post，它区别于我们今天所使用的get请求，但用法却是大同小异。

而对于后者，Requests Headers，我们把它称作请求头。它里面会有一些关于该请求的基本信息，比如：这个请求是从什么设备什么浏览器上发出？这个请求是从哪个页面跳转而来？

它最大的应用是帮助我们应对“反爬虫”技术，将Python爬虫伪装成真正的浏览器，不为服务器所辨识；同时也可以标记这个请求的来源是什么，最终帮助我们拿到想要的信息。

除此之外，我们还通过项目实操，学会如何判断我们想要的信息是在Html，还是在XHR里：