## 逢山开路与不甘庸碌.md
### 1.爬虫总复习
- 工具
- 工具解析与提取(一)
- 工具解析与提取(二)
- 更厉害的请求
- 储存
- 更多的爬虫
- 更多的爬虫更强大的爬虫-框架
- 给爬虫加上翅膀
### 2.爬虫进阶的路线指引
- 解析与提取
- 存储
- 数据分析与可视化
- 更多的爬虫
- 更强大的爬虫-框架
- 项目训练
### 3.反爬虫应对策略汇总

反爬虫应对策略汇总
说完了进阶路线指引，我们来说反爬虫。几乎所有的技术人员对反爬虫都有一个共识：所谓的反爬虫，从不是将爬虫完全杜绝；而是想办法将爬虫的访问量限制在一个可接纳的范围，不要让它过于肆无忌惮。

原因很简单：爬虫代码写到最后，已经和真人访问网络毫无区别。服务器的那一端完全无法判断是人还是爬虫。如果想要完全禁止爬虫，正常用户也会无法访问。所以只能想办法进行限制，而非禁止。

所以，我们可以了解有哪些“反爬虫”技巧，再思考如何应对“反爬虫”。

有的网站会限制请求头，即Request Headers，那我们就去填写user-agent声明自己的身份，有时还要去填写origin和referer声明请求的来源。

有的网站会限制登录，不登录就不给你访问。那我们就用cookies和session的知识去模拟登录。

有的网站会做一些复杂的交互，比如设置“验证码”来阻拦登录。这就比较难做，解决方案一般有二：我们用Selenium去手动输入验证码；我们用一些图像处理的库自动识别验证码（tesserocr/pytesserart/pillow）。

有的网站会做IP限制，什么意思呢？我们平时上网，都会有携带一个IP地址。IP地址就好像电话号码（地址码）：有了某人的电话号码，你就能与他通话了。同样，有了某个设备的IP地址，你就能与这个设备通信。

使用搜索引擎搜索“IP”，你也能看到自己的IP地址。


如上，它显示出了我的IP地址，所用的网络通信服务商是深圳电信。如果这个IP地址，爬取网站频次太高，那么服务器就会暂时封掉来自这个IP地址的请求。

解决方案有二：使用time.sleep()来对爬虫的速度进行限制；建立IP代理池（你可以在网络上搜索可用的IP代理），一个IP不能用了就换一个用。大致语法是这样：

```python
import requests
url = 'https://…'
proxies = {'http':'http://…'}
# ip地址
response = requests.get(url,proxies=proxies)
```
以上，就是市面上最常见的反爬虫策略，以及对应的应对策略。你会发现没什么能真正阻拦你。这正印证了那句话：所谓的反爬虫，从不是将爬虫完全杜绝；而是想办法将爬虫的访问量限制在一个可接纳的范围，不要让它过于肆无忌惮。
所谓的反爬虫，从不是将爬虫完全杜绝；而是想办法将爬虫的访问量限制在一个可接纳的范围，不要让它过于肆无忌惮。


需求三步: 确认目标-分析过程-先面向过程一行行实现代码-代码封装。
爬虫四步：获取数据-解析数据-提取数据-存储数据。
